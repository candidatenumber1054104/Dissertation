# importing required libraries
import pandas as pd
from pandas import DataFrame
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import plotly.express as px
from sklearn.preprocessing import StandardScaler
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot
%config InlineBackend.figure_format='retina'
from scipy.spatial.distance import cdist
from mpl_toolkits import mplot3d


# reading the data and looking at the first five rows of the data
File = 'try10.csv'
data = pd.read_csv(File, header=0)
data.head()

# statistics of the data
data.describe()

# load dataset
# retrieve just the numeric input values
data1 = data.values[:, :-1]
# perform a robust scaler transform of the dataset
trans = MinMaxScaler()
data = trans.fit_transform(data)
# convert the array back to a dataframe
dataset = DataFrame(data)
# summarize
print(dataset.describe())
# histograms of the variables
dataset.hist()
pyplot.show()

# Create a PCA instance: pca
n = len(dataset.columns)
pca = PCA(n_components=6)
principalComponents = pca.fit_transform(dataset)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)

kmeans_data = pd.read_csv('Countries and their clusters.csv', header = None)
kmeans_data
color_map = {0:'black', 1:'blue', 2:'red', 3:'green', 4:'cyan', 5:'magenta', 6:'crimson'}
color_list = [color_map[i] for i in kmeans_data[1]]
color_list


#plot biplots for PCA analysis
## project data into PC space

# 0,1 denote PC1 and PC2; change values for other PCs
xvector = pca.components_[0] # see 'prcomp(my_data)$rotation' in R
yvector = pca.components_[1]
zvector = pca.components_[2]

xs = pca.transform(dataset)[:,0] # see 'prcomp(my_data)$x' in R
ys = pca.transform(dataset)[:,1]
zs = pca.transform(dataset)[:,2]

scalex = 1.0/(xs.max() - xs.min())
scaley = 1.0/(ys.max() - ys.min())
scalez = 1.0/(zs.max() - zs.min())

fig = plt.figure(figsize = (16, 9))
ax = plt.axes(projection ="3d")
ax.grid(b = True, color ='black', linestyle ='-.', linewidth = 0.4, alpha = 0.7)

ax.set_ylim3d(-0.4,0.4)
ax.set_xlim3d(-0.4,0.4)
ax.set_zlim3d(-0.4,0.4)

ax.scatter3D(xs * scalex, ys * scaley, zs * scalez, color = color_list)


#plt.xlabel('PCA 0')
#plt.ylabel('PCA 1')

ax.set_xlabel('PCA 0', fontweight ='bold')
ax.set_ylabel('PCA 1', fontweight ='bold')
ax.set_zlabel('PCA 2', fontweight ='bold')

## visualize projections
    
## Note: scale values for arrows and text are a bit inelegant as of now,
##       so feel free to play around with them

#for i in range(len(xvector)):
## arrows project features (ie columns from csv) as vectors onto PC axes
#    plt.arrow(0, 0, xvector[i]*max(xs), yvector[i]*max(ys),
#              color='r', width=0.0005, head_width=0.0025)
#    plt.text(xvector[i]*max(xs)*1.2, yvector[i]*max(ys)*1.2,
#             list(dataset.columns.values)[i], color='b')

#for i in range(len(xs)):
## circles project documents (ie rows from csv) as points onto PC axes
#    plt.plot(xs[i], ys[i], '')
#    #plt.plot(xs[i], ys[i], 'bo')
#    #plt.text(xs[i]*1.2, ys[i]*1.2, list(data_df.index)[i], color='b')

plt.show()

## defining the kmeans function with initialization as k-means++
#kmeans = KMeans(n_clusters=6, init='k-means++')
#
## fitting the k means algorithm on scaled data
#kmeans.fit(dataset)

# inertia on the fitted data
#kmeans.inertia_

## fitting multiple k-means algorithms and storing the values in an empty list
#SSE = []
#for cluster in range(1,20):
#    kmeans = KMeans(n_jobs = -1, n_clusters = cluster, init='k-means++')
#    kmeans.fit(dataset)
#    SSE.append(kmeans.inertia_)

### converting the results into a dataframe and plotting them
#frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})
#plt.figure(figsize=(12,6))
#plt.plot(frame['Cluster'], frame['SSE'], marker='o')
#plt.xlabel('Number of clusters')
#plt.ylabel('Inertia')

# k means using 5 clusters and k-means++ initialization
kmeans = KMeans(n_jobs = -1, n_clusters = 6, init='k-means++', 
            max_iter=1000, n_init=100, verbose=0, random_state=1234)
kmeans.fit(dataset)
pred = kmeans.predict(dataset)

# inertia on the fitted data
kmeans.inertia_

frame = pd.DataFrame(dataset)
frame['cluster'] = pred
frame['cluster'].value_counts()

dataset["cluster"]=kmeans.labels_

frame

plt.scatter(dataset)

dataset.head(180)

dataset["cluster"].value_counts()

#dataset.to_csv("Try11results.csv", index=False)
datadick = pd.read_csv("datadick.csv")

#df = pd.DataFrame(dict(
#    r1={(1, 2, 3, 4, 5,6),(3,2,4,5,6,3)},
#    theta=['Energy Access (2019)','GDP Growth (2015-2019)','Governance Index (2019)',
#           '% of renewables (2015)', 'Practical Potential Solar', 'Wind Potential']))
#fig = px.line_polar(df, r='r1',theta='theta', line_close=True)
#fig.show()

#from sklearn.manifold import TSNE 
#model_10=Kmeans(6)
#tsne=TSNE(2) 
#arr=tsne.fit_transform(dataset)
#plt.scatter(arr[:,0],arr[:,1],c=model_10.labels_)

feature = ['Energy Access (2019)','GDP Growth (2015-2019)','Governance Index (2019)',
           '% of renewables (2015)', 'Practical Potential Solar', 'Wind Potential']
cluster1 = [1,3,2,4,1,6,1]
cluster2 = [3,6,5,5,3,1,3]
cluster3 = [4,3,2,1,2,5,4]
cluster4 = [5,2,5,6,1,3,5]
cluster5 = [2,1,6,2,3,6,2]
cluster6 = [4,2,5,3,2,1,4]

# Initialise the spider plot by setting figure size and polar projection
plt.figure(figsize=(15, 9))
plt.subplot(polar=True)



theta = np.linspace(0, 2 * np.pi, len(cluster1))

#Arrange the grid into number of sales equal parts in degrees
lines, labels = plt.thetagrids(range(0, 360, int(360/len(feature))), (feature))
 
# Plot 1st cluster graph
plt.plot(theta, cluster1)
 
# Plot 2nd cluster graph
plt.plot(theta, cluster2)

# Plot 2nd cluster graph
plt.plot(theta, cluster3)
 
# Plot 1st cluster graph
plt.plot(theta, cluster4)
 
# Plot 2nd cluster graph
plt.plot(theta, cluster5)

# Plot 2nd cluster graph
plt.plot(theta, cluster6)   

# Add legend and title for the plot
plt.legend(labels=('cluster1', 'cluster2', 'cluster3', 'cluster4', 'cluster5', 'cluster6'), loc=1)
#plt.title("Actual vs Expected sales by Employee")
 
# Dsiplay the plot on the screen
plt.show()

plt.savefig('trialtest.png')




#employee = ["Sam", "Rony", "Albert", "Chris", "Jahrum"]
#actual = [45, 53, 55, 61, 57, 45]
#expected = [50, 55, 60, 65, 55, 50]
# 
## Initialise the spider plot by setting figure size and polar projection
#plt.figure(figsize=(10, 6))
#plt.subplot(polar=True)
# 
#theta = np.linspace(0, 2 * np.pi, len(actual))
# 
## Arrange the grid into number of sales equal parts in degrees
#lines, labels = plt.thetagrids(range(0, 360, int(360/len(employee))), (employee))
# 
## Plot actual sales graph
#plt.plot(theta, actual)
#plt.fill(theta, actual, 'b', alpha=0.1)
# 
## Plot expected sales graph
#plt.plot(theta, expected)
# 
## Add legend and title for the plot
#plt.legend(labels=('Actual', 'Expected'), loc=1)
#plt.title("Actual vs Expected sales by Employee")
# 
## Dsiplay the plot on the screen
#plt.show()

# obtaining the centers of the clusters
centroids = kmeans.cluster_centers_
clusters = kmeans.fit_predict(data)

# points array will be used to reach the index easy
points = np.empty((0,len(dataset[0])), float)

# distances will be used to calculate outliers
distances = np.empty((0,len(dataset[0])), float)


for i, center_elem in enumerate(centroids):
    distances = []
    countrys = np.array(dataset[dataset['cluster'] == i].iloc[:, 0:6])
    for country in countrys:
        dist = [(country[i] - center_elem[i]) for i in range(len(center_elem))]
        dsq = [d**2 for d in dist]
        euc_dist = sum(dsq)**0.5
        distances.append(euc_dist)
    pd.DataFrame(distances).to_csv("centroid number try2" + str(i+1) + ".csv")


plt.figure(figsize=(10, 10))

colors = ['g', 'r', 'b', 'y', 'c','m','olive','b','m']
labels = labelsF
for klass, color in zip(range(0, 8), colors):
    plt.scatter(x=coords[labels == klass, 0], y=coords[labels == klass, 1],color=color, s=5, alpha=0.8)


